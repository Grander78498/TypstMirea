#import ("src/titul.typ"): *
#import ("src/preamble.typ"): *
#show: main

#titul(
  ..(
  Институт: "Информационных Технологий",
  Кафедра: "Вычислительной Техники", 
  Практика: "Отчёт по практическим работам", 
  Дисциплина: quoted("Проектирование и разработка нейронных сетей"), 
  Группа: "ИКБО-04-22", 
  Студент: "Егоров Л.А.",
  Должность: "старший преподаватель",
  Преподаватель: "Семёнов Р.Э."
  )
)
#show: template

#outline()

#heading("Введение", numbering: none)

Нейронные сети, вдохновленные структурой человеческого мозга, #lb совершили революцию в мире технологий, проникнув во все сферы нашей жизни. От распознавания лиц на смартфонах до прогнозирования погоды, от перевода языков до создания реалистичных изображений --- нейросети стали #lb неотъемлемой частью современного мира.

Нейронные сети активно применяются в различных областях, от медицины и финансов до развлечений и образования. За нейронными сетями лежит будущее во многих сферах жизнедеятельности общества.

Нейронные сети являются подклассом алгоритмов машинного обучения, и называются глубоким обучением. Цель данной дисциплины изучить принципы работы, архитектуры нейросетей и алгоритмы их оптимизации (обучения).

= Обучение по правилам Хебба

Правила Хебба, сформулированные в 1949, гласят: 
+ Если сигнал персептрона неверен и равен нулю, то необходимо #lb увеличить веса тех входов, на которые была подана единица.
+ Если сигнал персептрона неверен и равен единице, то необходимо уменьшить веса тех входов, на которые была подана единица.

В этой практической работе была построена модель перцептрона и обучена в соответствие с правилами Хебба.

== Входные данные

Для данной работы используется датасет с отказами сердца в #lb зависимости от различных данных о здоровье пациента. Часть датасета #lb представлена в Таблице @dataset.

#let results = csv("include/heart.csv")
#figure(table(
  columns: 14,
  align: center,
  table.header(..results.at(0).map(text.with(weight: "bold"))),
  ..results.slice(1, 4).flatten()
), caption: [Датасет heart.csv])<dataset>

Эти данные преобразованы с помощью Z-нормализации (@znorm).

$ z = frac(x - mu, sigma) $ <znorm>
#v(-18pt)

== Результат работы алгоритма

Нейрон обучен за 100 эпох с коэффициентом $alpha$, равным 0.0005. Результат обучения представлен на Рисунках @acc1 -- @test1. Код программы представлен в Листинге @hebb.

#figure(image("2024-12-21-20-31-48.png"), caption: [Изменение точности])<acc1>
#figure(image("2024-12-21-20-32-30.png"), caption: [Точность на тестовом датасете])<test1>

= Дельта-правило
  
Дельта правило является улучшенной версией алгоритма Хебба. По #lb сравнению со своим предшественником дельта правило имеет параметр, #lb отвечающий за скорость сходимости метода обучения, называемый скоростью обучения.

== Входные данные
Для данной работы используется датасет с отказами сердца в #lb зависимости от различных данных о здоровье пациента. Часть датасета #lb представлена в Таблице @dataset.

Эти данные преобразованы с помощью Minmax нормализации (@minmax).

$ X_(n o r m) = frac(X - X_(m i n), X_(m a x) - X_(m i n)) $<minmax>
#v(-18pt)

== Результат работы алгоритма

Нейрон обучен за 100 эпох с коэффициентом $alpha$, равным 0.0005. Результат обучения представлен на Рисунках @acc2 -- @test2. Код программы представлен в Листинге @delta.

#figure(image("2024-12-21-20-44-23.png", height: 30%), caption: [Изменение точности])<acc2>
#figure(image("2024-12-21-20-32-30.png.png"), caption: [Точность на тестовом датасете])<test2>

= Обратное распространение ошибки
Алгоритм обратного распространения ошибки (backpropagation) --- это метод обучения искусственных нейронных сетей, который используется для #lb минимизации ошибки предсказания путем корректировки весов сети. Он #lb работает в два этапа: прямое распространение сигнала для вычисления выхода сети и обратное распространение ошибки для обновления весов на основе #lb градиента функции потерь. Алгоритм эффективен и широко применяется в #lb задачах машинного обучения, таких как классификация и регрессия.

== Входные данные

Для данной работы используется датасет с отказами сердца в #lb зависимости от различных данных о здоровье пациента. Часть датасета #lb представлена в Таблице @dataset.

Эти данные преобразованы с помощью Minmax нормализации (@minmax).

== Результат работы алгоритма
Характеристики нейронной сети:
- количество скрытых слоёв: 2
- нейронов в первом скрытом слое: 6
- нейронов во втором скрытом слое: 3
- активационные функции в скрытых слоях: _relu_
- активационная функция на выходном слое: _sigmoid_
- скорость обучения: 0.01
- количество эпох: 100
Результат обучения представлен на Рисунках @acc3 -- @test3. Код программы представлен в Листинге @backprop.

#figure(image("2024-12-21-20-59-43.png", height: 35%), caption: [Изменение точности])<acc3>
#figure(image("error3.png"), caption: [Изменение ошибки])<err3>
#figure(image("test3.png"), caption: [Точность на тестовом датасете])<test3>
 
= Нейронные сети на радиально-базисных функциях
RBF-сети --- это тип нейронных сетей, который использует #lb радиально-базисные функции в качестве функций активации в скрытом слое. RBF-сети хорошо справляются с аппроксимацией нелинейных функций, и они менее чувствительны к шуму в данных.

== Входные данные
Для данной работы используется датасет с отказами сердца в #lb зависимости от различных данных о здоровье пациента. Часть датасета #lb представлена в Таблице @dataset.

Эти данные преобразованы с помощью Minmax нормализации (@minmax).

== Результат работы алгоритма
Характеристики нейронной сети:
- количество радиально-базисных функций: 10
- скорость обучения: 0.01
- количество эпох: 100
 
Результат обучения данной нейронной сети представлен на Рисунке @test4. Код программы представлен в Листинге @rbf

#figure(image("2024-12-22-10-21-50.png"), caption: [Точность RBF-сети])<test4>
 
= Карты Кохонена
SOM (Карты Кохонена) --- это тип нейронных сетей, который #lb используется для неконтролируемого обучения и визуализации данных. Карты самоорганизации работают на основе конкуренции между нейронами и подходят для группировки 2-мерных данных, такие как изображения. Также их можно применять и для данных больших размерностей, поскольку карта Кохонена #lb переведёт эти данные на двумерную сетку.

== Входные данные
Для данной работы используется датасет с отказами сердца в #lb зависимости от различных данных о здоровье пациента. Часть датасета #lb представлена в Таблице @dataset.

Эти данные преобразованы с помощью Minmax нормализации (@minmax).

== Результат работы алгоритма
Характеристики нейронной сети:
- размер сетки: $10 times 10$
- $sigma = 0.3$
- $op("lr") = 0.5$

Результат обучения представлен на Рисунке @sompic. Код программы представлен в Листинге @som.

#figure(image("som.png"), caption: [Карта Кохонена])<sompic>

= Нейронные сети встречного распространения
CPN - это тип нейронных сетей, используемых для решения задач классификации и аппроксимации функций. Они отличаются от традиционных нейронных сетей, таких как многослойные перцептроны, своим архитектурным строением и принципом работы.

== Входные данные
В качестве задачи выберем аппроксимацию функции, которая является отображением из экспоненциального распределения в нормальное распределение. Нейросеть будет иметь 3 входа и 3 выхода. Возьмем 1000 синтетически сгенерированных примеров и разобьем их на пакеты по 16 штук.

== Результат работы алгоритма
Скорость обучения для слоя Кохонена возьмем 0.7, для слоя Гроссберга 0.1, количество эпох 2. Кривая обучения представлена ниже.

 
Рисунок 6.2.1 – Кривая обучения нейросети
 
= Рекуррентные нейронные сети
Рекуррентные нейросети являются более современной архитектурой нейронных сетей. Их основным преимуществом является возможность запоминать предыдущие значения во входных данных, что является важным свойством для анализа последовательностей – текста, временных рядов, звука и тд. В данной работе реализуем рекуррентную сеть на базе 2 GRU слоев (100 и 50 нейронов) и между ними функцию активации ELU. 
 
== Входные данные
В качестве задачи возьмём регрессию. Синтетически сгенерируем набор данных, который содержит 25000 образца с 100 признаками. Все данные масштабируем к диапазону [0;1] по формуле:

Данные будем подавать в пакетах по 32 образца. Разделим данные на обучающую и тестовую выборки в соотношении 80% и 20% соответственно.

== Результат работы алгоритма
В качестве метрики выберем среднюю квадратичную ошибку, так как она чувствительна к большим ошибка и уменьшает влияние более маленьких ошибок. А скорость обучения выберем равную 0.010, количество эпох равное 25 и будем увеличивать скорость обучения на 10% каждую эпоху. Скрытые состояния будем сохранять в нейросети. На графиках ниже представлены кривые обучения нейросети (верхняя за цикл обучения, нижняя за цикл теста) – на оси абцисс – эпоха, на оси ординат – средняя ошибка за эпоху.
 
 
Рисунок 7.2.1 – Кривая обучения нейросети
 
= Свёрточные нейронные сети
Сверточные нейронные сети (CNN) - это тип нейронных сетей, специально разработанный для обработки данных с пространственной структурой, например, изображений, аудио и текстов. Ключевыми особенностями являются: слои свертки (извлекают ключевую информацию) и слой пулинга (уменьшают размерность данных, убирая избыточную информацию). В данной работе предлагает сделать вариационный автокодировщик (VAE) для генерации рукописных цифр. Главная особенность VAE в том, что кодировщик VAE переводит входные данные в латентное нормальное распределение, а декодировщик из нормального распределения восстанавливает первоначальные данные.

== Входные данные
Вариационный автокодировщик обучается без учителя, так как в процессе обучения он учится восстанавливать первоначальные данные, то есть на выходе у VAE должны быть входные данные. В качестве набора данных возьмем набор из 1797 одноцветных изображений рукописных цифр 8 на 8 пикселей. Входные данные приведем к диапозону [0;1] по формуле:
x_scaled=x/x_max 
Данные разобьем на пакеты по 16 образцов.

== Результат работы алгоритма
В качестве метрики выберем ELBO, которая состоит из 2 частей – ошибка реконструкции (в данном случае Log Loss) и расстояние Кульбака — Лейблера. А скорость обучения выберем равную 0.002, количество эпох равное 100. На рисунке ниже представлен результат генерации цифры из случайного вектора.
 
 
Рисунок 8.2.1 – Результат генерации рукописной цифры
 
#heading("Заключение", numbering: none)
В результате практических работ были изучены алгортимы и оптимизации нейронных сетей и их архитектуры.

Важно отметить, что глубокое обучение --- это динамично развивающаяся область. Постоянные исследования и разработки новых алгоритмов и архитектур нейронных сетей продолжаются, и полученные в рамках данной дисциплины знания являются отправной точкой для дальнейшего изучения и освоения новых технологий.

#appendix()

==== Реализация обучения по правилам Хебба
#simple-code(raw(read("include/neuron1.py")),
            "Код файла main.py",
            label: <hebb>)

==== Реализация дельта-правила
#simple-code(raw(read("include/neuron2.py")),
            "Код файла main.py",
            label: <delta>)

==== Реализация обратного распространения ошибки
#simple-code(raw(read("include/neuron3.py")),
            "Код файла main.py",
            label: <backprop>)

==== Реализация сети на радиально-базисных функциях
#simple-code(raw(read("include/neuron4.py")),
            "Код файла main.py",
            label: <rbf>)

==== Реализация карты Кохонена
#simple-code(raw(read("include/neuron5.py")),
            "Код файла main.py",
            label: <som>)

==== Реализация сети встречного распространения
#simple-code(raw(read("include/neuron6.py")),
            "Код файла main.py",
            label: <counterprop>)

==== Реализация рекуррентной нейронной сети
#simple-code(raw(read("include/neuron7.py")),
            "Код файла main.py",
            label: <rnn>)

==== Реализация свёрточной нейронной сети
#simple-code(raw(read("include/neuron8.py")),
            "Код файла main.py",
            label: <cnn>)